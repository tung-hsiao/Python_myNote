# -*- coding: utf-8 -*-

"""
pytorch.ipynb

Automatically generated by Colaboratory.

"""

import numpy as np
from torch.autograd import Variable
import torch
import torch.nn as nn

# Common batch normalization:
# num_features: C from an expected input of size (N, C, H, W)
# affine: a boolean value that when set to True, this module has
#         learnable affine parameters (gamma and beta). Default: True
# c = 0, mean and standard-deviation are calculated over the mini-batches
# c = 1, mean and standard-deviation are calculated over the mini-batches
# ...
# c = C-1, mean and standard-deviation are calculated over the mini-batches

BS = 2
C, H, W = 3, 2, 2
input = np.arange(BS*C*H*W)
input.resize(BS, C, H, W)
input = Variable(torch.from_numpy(input).clone().float()).contiguous()

norm = nn.BatchNorm2d(num_features=C, affine=False)
input = norm(input)

# Instance normalization:
# input: (N, C, H, W)
# swap dimension 0 and 1
input = np.arange(BS*C*H*W)
input.resize(BS, C, H, W)
input = Variable(torch.from_numpy(input).clone().float()).contiguous()

instanceNorm = nn.BatchNorm2d(num_features=BS, affine=False)
print(input)
input = input.transpose(0,1).contiguous()
print(input)
input = instanceNorm(input)
input = input.transpose(0,1).contiguous()

# a few operations on Tensors that do not change the contents of a tensor, 
# but change the way the data is organized. These operations include:
# narrow()
# view()
# expand()
# transpose()
# when you call transpose(), it doesn't generate a new tensor 
# with a new layout, it just modifies meta information in the Tensor object

# When you call contiguous(), it actually makes a copy of the tensor such 
# that the order of its elements in memory is the same as if it had been 
# created from scratch with the same data.
